# Neural Foundation Model Inference Docker Image
# Optimized for real-time neural decoding with minimal latency

FROM nvidia/cuda:11.8-runtime-ubuntu20.04

# Avoid interactive prompts during build
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1

# Install system dependencies (minimal for inference)
RUN apt-get update && apt-get install -y \
    python3.9 \
    python3.9-dev \
    python3-pip \
    curl \
    build-essential \
    libjpeg-dev \
    libpng-dev \
    libhdf5-dev \
    libfftw3-dev \
    && rm -rf /var/lib/apt/lists/*

# Set Python 3.9 as default
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 1
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.9 1

# Install pip and upgrade
RUN python3 -m pip install --upgrade pip setuptools wheel

# Install PyTorch with CUDA support (inference optimized)
RUN pip3 install torch==2.1.0+cu118 torchvision==0.16.0+cu118 torchaudio==2.1.0+cu118 \
    --index-url https://download.pytorch.org/whl/cu118

# Install inference-specific libraries
RUN pip3 install \
    fastapi==0.104.0 \
    uvicorn[standard]==0.24.0 \
    websockets==12.0 \
    pydantic==2.4.0 \
    numpy==1.24.0 \
    scipy==1.11.0 \
    scikit-learn==1.3.0

# Install neural data processing (minimal)
RUN pip3 install \
    mne==1.5.0 \
    h5py==3.9.0

# Install monitoring
RUN pip3 install \
    prometheus-client==0.18.0

# Set working directory
WORKDIR /workspace

# Copy only necessary files for inference
COPY requirements.txt .
COPY src/ src/
COPY scripts/real_time_inference.py scripts/
COPY configs/ configs/

# Install the package (inference mode)
RUN pip3 install -e . --no-deps

# Create non-root user
RUN useradd -m -s /bin/bash neural_user && \
    chown -R neural_user:neural_user /workspace
USER neural_user

# Set environment variables for inference
ENV PYTHONPATH="/workspace/src:${PYTHONPATH}"
ENV CUDA_VISIBLE_DEVICES="0"
ENV TORCH_CUDNN_V8_API_ENABLED=1
ENV TORCH_CUDNN_V8_API_LRU_CACHE_LIMIT=32

# Optimize for inference
ENV OMP_NUM_THREADS=4
ENV MKL_NUM_THREADS=4
ENV NUMEXPR_NUM_THREADS=4

# Expose ports
EXPOSE 8080 8081

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=30s --retries=3 \
    CMD curl -f http://localhost:8080/health || exit 1

# Default command
CMD ["python3", "scripts/real_time_inference.py", \
     "--model-path", "/workspace/models/foundation_model.pt", \
     "--task", "motor_control", \
     "--port", "8080", \
     "--host", "0.0.0.0"]